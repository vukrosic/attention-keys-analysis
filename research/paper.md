# QK-Norm Might Worsen Muon Optimizer LLM Training

> **Note:** This is an exploratory blog post documenting early-stage observations from a single-seed experiment. The findings are directional signals, not established results. We run one seed per condition and train for only 50M tokens (~0.3% of Chinchilla-optimal for a 1.5B model). All claims should be read as "we observed X" rather than "X is true in general."

---

## Key Findings at a Glance

![Key Rank Collapse â€” Does Î³ or Normalization Drive It?](./images/2_key_pr_causal.png)

Each attention head in a transformer uses a **128-dimensional space** to represent tokens. Ideally, the model spreads its representations across all 128 dimensions. In practice, many dimensions shrink to near-zero variance â€” the model "collapses" into a lower-dimensional subspace. The y-axis in the figure above measures how many of those 128 dimensions are actively carrying variance (higher = more dimensions in use). **Important:** near-zero variance doesn't mean a dimension is useless â€” it could still encode critical features. PR measures geometric spread, not information content.

**QK-Norm** is a technique that does two things to the attention vectors: (1) it **normalizes** them (controls their magnitude), and (2) it multiplies each dimension by a **learnable weight called Î³** â€” essentially giving the model a volume knob for each dimension. We wanted to know: when QK-Norm helps, which part is actually responsible?

If you need explanation of the QK-Norm and Î³ (gamma) scroll below first.

We trained a **1.5B parameter** (full specs below) language model three times â€” once with full QK-Norm, once with normalization but the Î³ knobs locked at 1.0 (so the model can't adjust them), and once with no QK-Norm at all. Here is what we found:

1. **The Î³ knobs â€” not the normalization â€” appear to control how many dimensions carry variance.** The orange line (full QK-Norm) recovers to 65.7 dimensions. The purple line (normalization but Î³ locked) recovers to only 59.6 â€” *worse* than the cyan line (no QK-Norm at all, 63.3). Locking the Î³ knobs makes the model *worse* than not normalizing in this single-seed experiment.

2. **But normalization â€” not Î³ â€” is what makes the model learn better.** Both normalized models (orange and purple) achieve nearly identical loss, and both beat the no-QK-Norm model. The Î³ knobs don't affect learning speed at all on this small scale, undertrained model.

- It seems that the model still learns to predict next token well, even if some dimensions inside of the heads are becoming near-zero (holding less data).

3. **Every model goes through the same collapse-then-recovery pattern.** All three start with ~87 dimensions in use, crash to ~51â€“55, then recover. This U-shape appears in all three conditions, suggesting it may be a general feature of early transformer training (though this needs to be investigated further).

4. **We cannot link PR to model quality.** This is the most important caveat: the model with the *fewest* active dimensions (Frozen Î³, PR=59.6) achieves the *best* loss (3.614). **Higher PR does not mean a better model in our experiment.** Î³ controls a geometric property (how evenly variance is spread) that we cannot yet connect to downstream performance. Whether collapsed dimensions represent wasted compute or efficient compression remains an open question â€” and the answer may well be "the model doesn't need all 128 dimensions."

Below we define every technical concept, then walk through the figures step by step.

---

### Concepts You Need First

#### What is Î³ (gamma)?

In a transformer attention head, each token is projected into a 128-dimensional **key** vector. **QK-Norm** applies RMSNorm to this vector, which does two things:

1. **Normalize** â€” divide every element by the root-mean-square of the whole vector, so the vector has a controlled magnitude.
2. **Scale by Î³** â€” multiply each of the 128 elements by its own learnable weight $\gamma_j$.

$$\text{RMSNorm}(\mathbf{x})_j = \gamma_j \cdot \frac{x_j}{\sqrt{\frac{1}{128}\sum_{i=1}^{128} x_i^2}}$$

Think of Î³ as a **per-dimension volume knob**. Each of the 128 dimensions gets its own knob, initialized to 1.0 (all equal). As training progresses, the model can turn some knobs up (amplify that dimension) and others down (suppress it), giving it direct control over which dimensions matter.

**Concrete example** with a 4-dimensional vector $\mathbf{x} = [2, 4, 6, 8]$:
- RMS = $\sqrt{(4 + 16 + 36 + 64)/4} = \sqrt{30} \approx 5.48$
- After normalization: $[0.365,\ 0.730,\ 1.095,\ 1.461]$ (all values rescaled)
- Now suppose training learns $\gamma = [1.5, 1.0, 0.5, 0.1]$:
  1. Dim 1: $0.365 Ã— 1.5 = 0.548$ â† **amplified**
  2. Dim 2: $0.730 Ã— 1.0 = 0.730$ â† unchanged
  3. Dim 3: $1.095 Ã— 0.5 = 0.548$ â† **dampened**
  4. Dim 4: $1.461 Ã— 0.1 = 0.146$ â† **nearly removed**
- Final: $[0.55,\ 0.73,\ 0.55,\ 0.15]$ â€” Î³ has reshaped which dimensions carry information.

#### What is Participation Ratio (PR)?

PR measures **how many of the 128 dimensions carry significant variance** in the key vectors. It answers: "is the model spreading variance across many dimensions, or concentrating it into a few?"

- **PR = 128** â†’ all dimensions contribute equally (maximum diversity, full rank)
- **PR = 1** â†’ strictly means only one singular value is nonzero and all others are exactly zero. In practice, singular values are never exactly zero â€” they are just very small. So a real model might show PR = 2â€“3 in a severe collapse, meaning variance is overwhelmingly concentrated in 2â€“3 dimensions while the remaining ~125 carry negligible (but not zero) variance.
- **PR â‰ˆ 60** (what we observe) â†’ roughly 60 of 128 dimensions carry significant variance

When PR drops, variance concentrates into fewer dimensions â€” many key dimensions shrink to near-zero variance. This is called **dimensional collapse** (or rank collapse). **Important caveat:** a low-variance dimension is not necessarily a useless dimension. It could still encode subtle but critical distinctions (e.g., binary features). PR measures geometric spread, not information content. Whether collapsed dimensions represent wasted compute or efficient compression remains an open question.

#### The Three Experimental Conditions

We train the same 1.5B model three times, changing only how QK-Norm is configured:

| Condition | Normalization? | Î³ learned? | Purpose |
|:---|:---:|:---:|:---|
| **Learned Î³** | âœ… Yes | âœ… Yes | Full QK-Norm â€” the real thing |
| **Frozen Î³=1** | âœ… Yes | âŒ No (locked at 1.0) | Ablation â€” has normalization but *not* the learned scaling |
| **No QK-Norm** | âŒ No | âŒ No | Baseline â€” no normalization at all |

**Why three conditions?** Because QK-Norm does two things at once (normalize + scale by Î³). With only two conditions (on/off) we can't tell which part matters. The Frozen Î³=1 condition surgically removes Î³ learning while keeping normalization, letting us isolate the cause.

---

### Figure 1: The Central Result â€” Does Î³ or Normalization Drive Rank?

![Key Rank Collapse â€” Does Î³ or Normalization Drive It?](./images/2_key_pr_causal.png)

**What this figure shows:** Lower PR (y-axis) possibly indicates more wasted compute or useless dimensions, but this needs further investigation, so let's define it strictly: The y-axis is *Participation Ratio (PR)* is the number of dimensions carrying significant variance (out of 128). Higher means variance is more evenly spread; lower means it is concentrated into fewer dimensions. The x-axis is training progress in millions of tokens.

**Reading it step by step:**

1. **All three models start at PR â‰ˆ 87** (top-left). At random initialization, each of the 128 dimensions contributes roughly equally â€” the model hasn't learned anything yet.

2. **All three crash to PR â‰ˆ 51â€“55 by ~8M tokens** (bottom of the U-curve). This is *rank collapse* â€” the model destroys its initial random structure as it begins learning. This happens regardless of whether QK-Norm is used.

3. **After the collapse floor, the three lines diverge** â€” this is where the experiment reveals its answer:
   - ðŸŸ  **Learned Î³ (orange)** recovers the fastest and highest, reaching PR = **65.7**
   - ðŸ”µ **No QK-Norm (cyan)** recovers to PR = **63.3** â€” surprisingly, the second-best
   - ðŸŸ£ **Frozen Î³=1 (purple)** recovers the least, only to PR = **59.6**

4. If normalization were the key mechanism for recovery, the two normalized models (Learned Î³ and Frozen Î³) would recover similarly. Instead, removing Î³ learning (Frozen) makes it *worse* than having no normalization at all. **In this experiment, the learned Î³ parameter â€” not the normalization â€” appears to be what drives rank recovery.** (Caveat: this is a single-seed result; the 6-unit difference could narrow or widen with different seeds.)

### Figure 2: Loss Tells a Different Story

![Training & Validation Loss](./images/1_loss.png)

**What this figure shows:** Training and validation loss (lower = better language modeling) over the same 50M tokens.

**Reading it step by step:**

1. **Both normalized models (Learned Î³ and Frozen Î³) achieve nearly identical loss** â€” around 3.62 for training, 3.44 for validation. They overlap almost perfectly.

2. **The No QK-Norm model has noticeably higher loss** â€” 3.68 training, 3.51 validation. Normalization clearly helps the model learn better.

3. **But here's the key insight:** Compare this to Figure 1 above. For *loss*, normalization helps equally whether Î³ is learned or frozen. For *rank*, only learned Î³ helps â€” frozen Î³ actually hurts. **This suggests loss and rank are driven by different mechanisms:**
   - **Normalization â†’ stabilizes gradients â†’ lowers loss** (doesn't need Î³)
   - **Learned Î³ â†’ selectively amplifies/suppresses dimensions â†’ maintains dimensional diversity** (needs Î³)

4. **An open question emerges:** The Frozen Î³=1 model has the *lowest* PR (59.6) but achieves the *best* loss (3.614). If the collapsed dimensions contained critical information, we would expect worse loss. This suggests either (a) those dimensions are genuinely redundant at this training stage, (b) 50M tokens is too early for the rank difference to manifest in loss, or (c) PR captures geometric properties that don't directly map to task-relevant information. We cannot yet determine which explanation is correct.

### The Three Main Observations, Explained

> **Observation 1: "Î³ appears to drive dimensional diversity, normalization drives loss."**
>
> *What this means:* In our experiment, QK-Norm does two things, and they appear to be independent. The normalization step (dividing by RMS) helps the model train faster (lower loss). The learned Î³ weights help the model maintain higher effective dimensionality in the key space. **However, we cannot yet link higher PR to better model quality** â€” at 50M tokens, Frozen Î³ achieves the *lowest* PR but the *best* loss. Î³ controls a geometric property that may or may not matter for downstream tasks.

> **Observation 2: "PR follows a U-shaped trajectory."**
>
> *What this means:* Every model â€” with or without QK-Norm â€” goes through the same three phases: (1) rank collapses as random initialization is destroyed, (2) rank recovers as the model learns meaningful structure, (3) rank plateaus. We observe this in all three conditions, suggesting it may be a general feature of early transformer training.

> **Observation 3: "Normalization without Î³ appears counterproductive for rank."**
>
> *What this means:* If you apply QK-Norm but freeze Î³ at 1 (so every dimension is treated equally), you actually get *worse* rank than not using QK-Norm at all in our experiment. One possible explanation: the normalization projects all key vectors onto a sphere, which constrains the geometry. Without Î³ to selectively stretch dimensions, this constraint *limits* the model's ability to differentiate dimensions, causing more collapse than if you just left the keys unnormalized. We have not verified this mechanistic explanation â€” it is a hypothesis consistent with the data.

### Additional Observation: Depth-PR Gradient Inversion

> In a pilot study at 500K tokens, we observed the standard depth-PR gradient: early layers had higher rank (~122) and deep layers had lower rank (~106), with a roughly linear decline of ~0.47 PR units per layer. This is consistent with deeper layers receiving stronger gradient signal from the loss and reorganizing their spectra faster.
>
> By 50M tokens, this pattern **inverts** â€” deeper layers develop *higher* PR than shallow layers. The model's internal structure appears to reorganize as training progresses, with later layers developing more complex, higher-dimensional representations over time.
>
> **Caveat:** This comparison is between two separate experiments (a 500K pilot and this 50M run) with different evaluation batch sizes and slightly different configurations. We present this as an interesting observation, not a rigorous finding. A proper analysis would require tracking per-layer PR continuously within a single run, which our data supports (see Section 3.2).

---

## 1. Overview

This post investigates how **QK-Normalization** (RMSNorm applied to both query and key projections) affects dimensional collapse in transformer attention heads. In our Gemma-style architecture, RMSNorm is applied to **both Q and K** independently before rotary position encoding â€” hence the name "QK-Norm."

We train a **1.5B parameter** dense LLM for **~49M tokens** under the three conditions described above to disentangle the effect of normalization from the effect of the learned scale parameter ($\gamma$). This is an ablation study with a single seed per condition â€” our findings are preliminary observations that require replication.

### Architecture & Training Config

| Parameter | Value |
|:---|:---|
| **Model Size** | ~1.58B Parameters |
| $d_\text{model}$ | 2048 |
| Layers | 32 |
| Attention Heads | 16 ($d_k = 128$) |
| KV Heads | 8 (GQA) |
| $d_\text{ff}$ | 8192 |
| **Training Budget** | **~49M Tokens** per condition |
| **Hardware** | NVIDIA H100 80GB |
| Batch Size | 8 |
| Grad Accumulation | 4 |
| Effective Batch | $8 \times 4 \times 2048 = 65{,}536$ tokens/step |
| Optimizer | Muon + AdamW |
| Dataset | FineWeb/Cosmopedia Mix (1B subset) |

---

## 2. Results Summary

| Metric | Learned Î³ | Frozen Î³=1 | No QK-Norm |
|:---|:---:|:---:|:---:|
| **Initial Mean PR** | 86.85 | 86.85 | 86.42 |
| **Minimum Mean PR** | 55.17 (7.9M) | 51.84 (9.8M) | 51.28 (7.9M) |
| **Final Mean PR (49M)** | **65.70** | 59.55 | 63.25 |
| **Final Train Loss** | **3.618** | **3.614** | 3.683 |
| **Final Val Loss** | **3.440** | **3.439** | 3.505 |

## 4. Mathematical Background

### 4.1 QK-Normalization

In standard attention, a token representation $\mathbf{x} \in \mathbb{R}^{d_\text{model}}$ (a 2048-dimensional vector representing one token) is projected into queries and keys:

$$Q = XW_Q, \quad K = XW_K$$

where:
- $X \in \mathbb{R}^{n \times d_\text{model}}$ is the input matrix ($n$ tokens, each a 2048-dim vector)
- $W_Q \in \mathbb{R}^{d_\text{model} \times d_k}$ is the query weight matrix (projects from 2048 â†’ 128 dimensions per head)
- $W_K \in \mathbb{R}^{d_\text{model} \times d_k}$ is the key weight matrix (same shape)
- $d_k = 128$ is the per-head dimension

With QK-Norm (as in our Gemma-style model), RMSNorm is applied to **both Q and K independently**, before rotary position encoding (RoPE):

$$Q = \text{RoPE}\!\left(\text{RMSNorm}(XW_Q)\right), \quad K = \text{RoPE}\!\left(\text{RMSNorm}(XW_K)\right)$$

The RMSNorm operation for a single vector $\mathbf{x} \in \mathbb{R}^{d_k}$ works as follows:

$$\text{RMS}(\mathbf{x}) = \sqrt{\frac{1}{d_k}\sum_{i=1}^{d_k} x_i^2}$$

$$\text{RMSNorm}(\mathbf{x})_j = \frac{\gamma_j \cdot x_j}{\text{RMS}(\mathbf{x})}$$

where:
- $\text{RMS}(\mathbf{x})$ is a single scalar â€” the root-mean-square of all 128 values in the vector
- $x_j$ is the value at dimension $j$ (before normalization)
- $\gamma_j$ is a learnable scalar weight for dimension $j$, initialized to 1.0
- The output at dimension $j$ is the original value divided by the RMS, then scaled by $\gamma_j$

**Scope of Î³:** Each RMSNorm layer has its **own independent** $\gamma \in \mathbb{R}^{128}$ vector. Since QK-Norm applies RMSNorm to both Q and K at every layer, there are $32 \text{ layers} \times 2 \text{ (Q and K)} = 64$ separate $\gamma$ vectors in the model, totaling $64 \times 128 = 8{,}192$ additional learnable parameters.

**What Î³ controls:** At initialization ($\gamma = \mathbf{1}$), the norm simply rescales the vector to have unit RMS. As training progresses, individual $\gamma_j$ values diverge â€” some grow above 1 (amplifying that dimension) and some shrink toward 0 (suppressing it). This gives the model a direct, per-dimension knob to control which dimensions of the key/query space are important.

**What RoPE does:** Rotary Position Encoding ($\text{RoPE}$) applies a position-dependent rotation to pairs of dimensions in the Q and K vectors. It encodes positional information without adding parameters. It is applied **after** RMSNorm, so $\gamma$ acts on the pre-RoPE representation.

### 4.2 Why This Matters for Rank

The **Participation Ratio (PR)** measures how evenly variance is distributed across the 128 dimensions of each attention head's key space. It is a geometric measure of spectral spread, not a direct measure of information content.

To compute it, we collect the key vectors $K \in \mathbb{R}^{n \times 128}$ from many tokens and compute the Singular Value Decomposition (SVD), yielding singular values $\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_{128}$. These singular values tell us how much variance each dimension captures:

$$\text{PR}(K) = \frac{\left(\sum_{i=1}^{d_k} \sigma_i\right)^2}{\sum_{i=1}^{d_k} \sigma_i^2}$$

**Intuition:**
- If all 128 singular values are equal ($\sigma_i = c$), then $\text{PR} = 128$ â€” all dimensions contribute equally (full rank)
- If only one singular value is nonzero ($\sigma_1 = c$, rest = 0), then $\text{PR} = 1$ â€” total collapse to a single dimension
- A PR of 60 (roughly what we observe at convergence) means the information is spread across roughly 60 effective dimensions

**How each condition affects rank:**
- **Without QK-Norm**: The singular values of $K$ are determined solely by $W_K$ and the input data. The model can only control rank through the weight matrix.
- **With QK-Norm (Learned Î³)**: The explicit $\gamma$ parameter can independently amplify or suppress each dimension â€” giving the model a direct, low-resistance path to shape the spectrum.
- **With QK-Norm (Frozen Î³=1)**: Normalization constrains key norms but without per-dimension flexibility, creating a geometric constraint that limits spectral diversity.

## 5. Î³ Dynamics

The Î³ coefficient of variation (CV) reveals how the model uses the learnable parameter:

- **High Î³ CV layers** (L0: 0.169, L15: 0.155, L27: 0.172): Aggressive dimension differentiation, correlated with higher PR recovery.
- **Low Î³ CV layers** (L5â€“L10: 0.07â€“0.08): Minimal differentiation, correlated with the lowest PR values.
- **Frozen Î³=1**: CV = 0.0 at all layers (confirming ablation correctness).

### 5.1 Final Î³ Values: What Did the Model Learn?

We saved the final Î³ values for all 32 layers at the end of training (49M tokens). Here is what we found:

**Î³ values are mostly > 1.0 â€” the model amplifies rather than suppresses.** The global mean Î³ across all layers is ~1.19. Only Layer 0 has a mean Î³ below 1.0 (mean = 0.913). All other layers have mean Î³ between 1.04 and 1.37. This is the opposite of what we initially expected â€” rather than using Î³ to zero out unwanted dimensions, the model uses it to **amplify** most dimensions, with selective per-dimension variation around that elevated baseline.

| Layer Zone | Layers | Mean Î³ | Interpretation |
|:---|:---:|:---:|:---|
| **Shallow** | 0â€“7 | 1.084 | Closest to initialization, Layer 0 is the outlier |
| **Middle** | 8â€“23 | 1.298 | Strongest amplification |
| **Deep** | 24â€“31 | 1.238 | Slightly less than middle layers |

**Layer 0 is uniquely asymmetric.** Layer 0 shows a striking pattern: the first 64 dimensions (which correspond to high-frequency RoPE rotations) have a mean Î³ of 1.017, while the last 64 dimensions (low-frequency RoPE rotations) are suppressed to a mean of 0.810. Layer 0 has 14 dimensions with Î³ < 0.7 â€” all of them in the last 64 dims. No other layer shows this asymmetry. This suggests the first layer learns to de-emphasize positional information carried by low-frequency RoPE components.

**Î³ differentiation correlates with PR recovery.** The layers with the most non-uniform Î³ distributions (highest CV: L27=0.172, L0=0.169, L15=0.155) tend to be the layers with the most distinct PR behavior. Layers with nearly uniform Î³ (lowest CV: L9=0.068, L10=0.072, L5=0.078) show minimal dimension differentiation.

**Key limitation:** We only have Î³ values at the *final* checkpoint. We cannot track how Î³ evolved during training â€” did suppressed dimensions start low and stay low, or did they start at 1.0 and gradually decline? Tracking individual Î³ trajectories would reveal whether dimension selection happens early (during collapse) or late (during recovery). This is a clear next step for future work.

## 6. Limitations

1. **Single Seed**: We run one seed per condition. The effect size (2â€“6 PR units) could easily shift or reverse with different random seeds. Without variance estimates, none of our PR differences are statistically confirmed.
2. **Early Training**: 50M tokens is ~0.3% of Chinchilla-optimal for a 1.5B model. We observe early-phase dynamics, not convergence behavior. The U-shaped trajectory and plateau could be transient phenomena.
3. **Key-Only Analysis**: We measure the PR of Keys ($K$). The effective rank of the full attention matrix ($QK^T$) also depends on Queries ($Q$), which we do not probe. It is possible that Q compensates for K collapse, or that the rank of $QK^T$ tells a different story.
4. **Muon Optimizer**: Muon's orthogonal updates may interact with spectral dynamics in ways specific to this optimizer. All findings may be Muon-specific artifacts.
5. **PR does not predict model quality**: This is perhaps the most important limitation. Participation Ratio captures how evenly variance is spread across dimensions â€” it does not measure how much task-relevant information each dimension carries. **Our own data shows that lower PR is associated with equal or better loss** (Frozen Î³=1 has the lowest PR and the best loss). Until we can establish a link between PR and downstream model quality, the practical significance of Î³ driving higher PR remains unknown.
6. **No singular value distributions**: We compute PR from SVD but do not save the full singular value spectrum. This means we cannot determine whether Î³ creates a gradual taper vs. a sharp cutoff in the eigenspectrum, which would help distinguish "efficient compression" from "dead dimensions."
7. **No downstream evaluation**: We measure loss but do not evaluate on any benchmark or downstream task.


### Future Work

The following would strengthen or modify these findings:

- **Multiple seeds** (â‰¥3) for variance estimation â€” the single most important next step. Our PR differences (2â€“6 units) may be within noise.
- **Extend training to 250Mâ€“1B+ tokens** to determine whether the PR plateau is stable, whether the PR/loss dissociation resolves, and whether Î³'s geometric control eventually translates to quality differences.
- **Measure query PR** alongside key PR, and compute the effective rank of $QK^T$ directly, to get a complete picture of attention head dimensionality.
- **Save full singular value distributions** at training checkpoints â€” this would reveal whether Î³ creates a smooth taper or a sharp rank cutoff, which matters for understanding the mechanism.
- **Track individual Î³ trajectories** over training to determine when dimension selection happens (during collapse, during recovery, or continuously).
- **Pure AdamW baseline** to determine how much of the observed dynamics are specific to Muon's orthogonal updates.
- **Downstream benchmark evaluation** to test whether PR differences at 50M tokens correspond to quality differences on actual tasks.
