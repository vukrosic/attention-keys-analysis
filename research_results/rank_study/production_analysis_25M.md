# Dimensional Collapse Analysis: 25M Token Scale-Up

This report analyzes the long-term convergence of the **Muon optimizer** experiments at the 25,000,000 token scale, comparing the impact of **QK-Normalization** on structural representation rank.

## üìä Long-Term Trajectory (25M Tokens)

The scale-up to 25M tokens revealed a **counter-intuitive discovery**: While QK-Normalization (A1) appeared more stable at 1M tokens, it leads to massive dimensional collapse at scale.

![25M Trajectory](pr_trajectory_4way.png)

*Figure 1: Comparison of Muon (A1, A2) vs. AdamW (B1, B2) over 25M tokens.*

### Key Observations:
1.  **The Muon Advantage:** Muon without QK-Norm (**A2**) is the clear structural winner, stabilizing at a PR of **~51**. Even the collapsed Muon run (A1, PR ~30) remains better than the best AdamW run.
2.  **The AdamW Collapse:** AdamW with QK-Norm (**B1**) is the most unstable, crashing to a PR of **~16**. This indicates that the representations are effectively inhabiting only 25% of the available semantic space.
3.  **The "Recovery" Phenomenon (A2):** Only Muon (A2) shows a clear "rank reclamation" phase after 1M tokens. AdamW (B2) shows a slight recovery but remains significantly handicapped.

## üìê Layer-Wise Final State (25M)

The divergence at the layer level is even more dramatic.

### A1: Baseline (Muon + QK-Norm) - Aggressive Collapse
![A1 Layer Rank](layer_rank_A1_Baseline_QK_Muon.png)
*Figure 2: Layer-wise PR for A1 at 25M tokens. Note the extreme collapse in Layer 1 and 3 (PR < 10).*

In A1, the model has functionally "abandoned" several layers, reducing them to highly axis-aligned, low-rank subspaces. This indicates that QK-Norm + Muon might allow the model to over-optimize specific attention heads into "fixed switches" far earlier than expected.

### A2: No QK-Norm (Muon Only) - Structural Diversity
![A2 Layer Rank](layer_rank_A2_NoQK_Muon.png)
*Figure 3: Layer-wise PR for A2 at 25M tokens. High rank (PR > 45) is maintained across ALL 22 layers.*

In A2, the rank is remarkably uniform. Every single layer is contributing significantly to the representational bandwidth. This configuration achieves a **Structural Equilibrium** where Muon's orthogonalization pressure perfectly balances the training signal's tendency to compress.

## üí° Research Conclusion: The QK-Norm Trap

Our 25M token study demonstrates that **QK-Normalization is a catalyst for dimensional collapse in transformers.**

While QK-Norm is intended to stabilize attention by preventing logits from exploding, it provides a "constrained hypersphere" that optimizers‚Äîespecially AdamW‚Äîcan easily exploit. 
*   **With AdamW:** The interplay between coordinate-wise adaptation and normalized features creates a path of least resistance towards one-hot, axis-aligned representations.
*   **With Muon:** The optimizer's native orthogonalization pressure acts as a better regularizer than explicit normalization. Muon without QK-Norm (A2) achieves a superior structural equilibrium that explicit normalization actually destroys.

**Final Recommendation:** For high-performance LLM training with the Muon optimizer, **disable QK-Normalization** to preserve representational bandwidth and prevent the middle-layer bottleneck.

---
*Results generated by `RankProbe` on 2026-02-18.*
