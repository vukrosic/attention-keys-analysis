【研究观察：QK-Norm 与 Muon 优化器】

今天我发现了一些奇怪的结果 —— QK-Norm + Muon 优化器虽然导致了更好的损失函数（loss）表现，但浪费了更多的计算资源（导致了更低秩的注意力头）。

实验设置：88M 参数 LLM，22 层，64 维注意力头，Muon 优化器。两次完全相同的实验 —— 一次在查询（queries）和键（keys）上使用了 QK-RMSNorm，另一次则没有。

起初，QK-Norm 的损失函数看起来更好，但请看“参与比 (Participation Ratio, PR)” —— 这是一个衡量每个注意力头 64 个可用维度中实际使用了多少维度的指标。两者的 PR 都在下降，但使用了 QK-Norm 的模型塌缩得更快。到 16M token 时，QK-Norm 模型的有效维度比不使用它的版本少了约 7%。

塌缩发生在何处？
这种塌缩在不同层之间并不是均匀的。在 Muon + QK-Norm 的实验中，某些层基本上已经“死”了。第 1 层和第 3 层的 PR 降到了 10 以下。在 64 个可能的维度中，这些层只使用了大约 10 个。剩下的就是我们所谓的“幽灵计算” (ghost compute) —— GPU 正在对那些几乎没有任何贡献的维度进行数学运算。

相比之下，不带 QK-Norm 的 Muon 表现均匀且具有活力。每一层的 PR 都保持在 45 以上，没有哪一层“死掉”，整个网络的表征带宽非常均匀。

悖论：
结构上看起“更差”的模型实际上在预测 token 时表现略好。但它是否构建了一些脆弱的东西？

待解决的问题：
- 结构的优势最终会在大规模训练中转化为更好的损失函数吗？
- QK-Norm 是否在“作弊”？它是否找到了一些低秩的快捷方式，虽然最小化了交叉熵，但牺牲了某些下游能力（如推理或泛化能力）？
- Muon 的正交化压力是否已经在做 QK-Norm 所做的事情了？

这可能是 Muon 的正交化压力与 QK-Norm 的几何约束压力之间的博弈。在没有归一化层的情况下，Muon 可以自由地推向高秩配置，使其内部表征保持多样性和分布性。

特别感谢 Novita AI 为本研究提供计算资源。
